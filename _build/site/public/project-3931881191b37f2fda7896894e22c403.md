# Introduction

Running train.py using my laptop would (according to my calculations) have taken about 3 days. Instead, I will be using google colab to run the notebook files, which I will then download and add in the Notebooks folder of this project. All notebooks can be accessed under 'supporting documents'.

This website is a [Myst document](https://mystmd.org/), which allows me to write about some of the changes I made to the `train.py` file in a structured way, combining the outputs of codecells with markdown text. For any figure, the `source : ...` will be displayed in the top right corner. Clicking on this will navigate the page to the source notebook file. 

The goal of this project is to: 
> Your task is to analyze and improve the training run of a small GPT-2 style model. The code is designed to train on a Hacker News headline dataset. Your goal is to minimize the validation loss as low as possible within 7 epochs. The baseline to beat is a validation loss of 1.754.

Where we want to:
- Check what sort of inputs the model receives, and the type of outputs we are looking for.
- Minimise the validation loss. 



# Initial run

First step is to run the orignial `trai.py` file as provided on the GitHub page.

:::{figure} #loss_first_run
:label: label_loss_first_run
:::

[](#label_loss_first_run) shows the loss as training progresses, converging towards `loss=1.753273` after 7 epochs. We can also have a look at an example of the output provided by the model. For this, we will look at `batch = 0` and `time = 10`. The decoded input to the model equals `Motorola Xoom commercial reminiscent of Apple's`, and the output tokens are displayed in [](#label_token_frequency_first_run) (top 10 probabilities).

:::{figure} #token_frequency_first_run
:label: label_token_frequency_first_run
:::

As seen in the figure, the model incorrectly predicts `<eos>`. For each input, the model would do this. After looking at the frequency of words used in the data, as displayed below in [](#token_frequency_input), we can see this matches.

:::{figure} #token_frequency_input
:label: label_token_frequency_input
:::

so the goal is not only to improve on the validation loss, but also to see 

# Step one - AdamW Optimiser 

The current model uses a SGD optimiser, which is less suitable for training attention blocks. I have implemented AdamW instead, which is commonly used by LLM's. 

```python
# Add different optimiser, which is better suited for transformer models
def get_optimizer(model, args):
    # Add AdamW as an optimiser
    return torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, betas=args.betas)
```

I have decided to add a `get_optimizer` function, which easily allows me to display the changes I made at the top of the code-cell. I have also decreased the learning rate, and added betas. 

```python
n_layer: int = 6
n_head: int = 8
d_model: int = 512
dropout: float = 0.1
lr = 5e-4                   #lr: float = 6e-3, 
weight_decay: float = 0.0
evals_per_epoch: int = 3
betas = (0.9, 0.95)         # Added betas for AdamW
```



# Step two - Positional context/increased vocab

Next, I realised the context can consist of multiple titles, where older titles are not relevant to current tokens being generated. An example of this is when we look at the first batch, which contains an input like this:  

> `Motorola Xoom commercial reminiscent of Apple's infamous jab at IBM<eos> OpenSSH Key Shielding<eos> Contextual Word Representations: A Contextual Introduction (2020)<eos> Windows 8 Should Be Free<eos> GNOME Developers `

The word comming after GNOME Developers does not depend on the previous generated titles. Positional information should provide guidence for the model to focus on the current sentence. I have added [Rotary Position Embedding (RoPE) ](https://towardsdatascience.com/positional-embeddings-in-transformers-a-math-guide-to-rope-alibi/), which I was unfamiliar with before this project. Additionally, I increased the vocab size for the tokeniser as tasks with a lot of unique words generally perform better with more unique tokens. 

After training, the output distribution for the same input as [](#label_token_frequency_first_run) is displayed below. 

:::{figure} #output_step_two_token_10
:label: label_output_step_two_token_10
:::



Another interesting, supposed to be snapchat: 

:::{figure} #token_predictions_two
:label: label_token_predictions_two
:::

# Concluding thoughts 

As seen in both figures, it will be hard to get significant improvements as there is a lack of context. Why should "Snapchat" be the first word? -> next step is to add more context

