{"version":2,"kind":"Notebook","sha256":"9bf10b729f88554b9ae16df6290697993e8f6768e5d4b69d078993cc35c14c6f","slug":"maincode-first-run","location":"/Notebooks/Maincode_first_run.ipynb","dependencies":[],"frontmatter":{"title":"Mainrun notebook - First run","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3"},"authors":[{"id":"Luuk Fröling","name":"Luuk Fröling"}],"date":"2025-11-17","github":"https://github.com/luukfroling/BEP","exports":[{"format":"ipynb","filename":"Maincode_first_run.ipynb","url":"/Maincode_first_run-09c6bc14ad735ffedf8be203e4e2b0ef.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{"id":"SO-sdKA7BOXM"},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"This is the first run of the code, without any changes. This is what I will be improving upon in the following notebooks.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"DxGYEY0Di9"}],"key":"gomewk8dKN"}],"identifier":"so-sdka7boxm","label":"SO-sdKA7BOXM","html_id":"so-sdka7boxm","key":"OvcpH5zV7d"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"ES-JH26zAVwu","outputId":"aa54ce56-201e-46a6-dc79-85c5636938ac","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"!pip install torch --index-url https://download.pytorch.org/whl/cu121\n!pip install datasets tokenizers structlog tqdm utils","identifier":"es-jh26zavwu-code","visibility":"show","enumerator":"1","html_id":"es-jh26zavwu-code","key":"AVsMboPp3n"},{"type":"output","id":"XT7k3mRhzh5TkrLUNaUG7","data":[{"name":"stdout","output_type":"stream","text":"Looking in indexes: https://download.pytorch.org/whl/cu121\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\nCollecting structlog\n  Downloading structlog-25.5.0-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\nCollecting utils\n  Downloading utils-1.0.2.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nDownloading structlog-25.5.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: utils\n  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=daf820524ae548e2186052cc0c5e842211e7d3f67d78349bd4eaa356fa09142b\n  Stored in directory: /root/.cache/pip/wheels/b6/a1/81/1036477786ae0e17b522f6f5a838f9bc4288d1016fc5d0e1ec\nSuccessfully built utils\nInstalling collected packages: utils, structlog\nSuccessfully installed structlog-25.5.0 utils-1.0.2\n"}],"identifier":"es-jh26zavwu-output","visibility":"hide","html_id":"es-jh26zavwu-output","key":"IccCSs1xlM"}],"identifier":"es-jh26zavwu","label":"ES-JH26zAVwu","html_id":"es-jh26zavwu","visibility":"show","key":"Ci4x2zpQmW"},{"type":"block","kind":"notebook-code","data":{"id":"4uS7VSPPCVkB"},"children":[{"type":"code","lang":"python","executable":true,"value":"import utils\nimport math, random, time\nfrom dataclasses import dataclass\nimport json\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\nfrom tqdm import tqdm\nimport structlog","identifier":"4us7vsppcvkb-code","enumerator":"2","html_id":"id-4us7vsppcvkb-code","key":"w6c367JWTV"},{"type":"output","id":"KWWJkL3rqGIDw0OI7OebX","data":[],"identifier":"4us7vsppcvkb-output","html_id":"id-4us7vsppcvkb-output","key":"Lr8e3Eym4a"}],"identifier":"4us7vsppcvkb","label":"4uS7VSPPCVkB","html_id":"id-4us7vsppcvkb","key":"K49rIHxx3u"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1THg0JBJBsc4","outputId":"7c03cbbc-ef76-4c46-d2d6-5632576ccb6d"},"children":[{"type":"code","lang":"python","executable":true,"value":"# CODE ADDED BY ME FOR LOGGING\n\nloss_tracker = [] #Plotting the loss will allow us to nicely display the loss in a Jupyter Book.\n\n# CODE FROM GITHUB\n@dataclass\nclass Hyperparameters:\n    block_size: int = 128\n    batch_size: int = 64\n    vocab_size: int = 16_000\n    n_layer: int = 6\n    n_head: int = 8\n    d_model: int = 512\n    dropout: float = 0.1\n    lr: float = 6e-3\n    weight_decay: float = 0.0\n    evals_per_epoch: int = 3\n\n    epochs: int = 7\n    seed: int = 1337\n    num_titles: int = 100_000\n    val_frac: float = 0.10\n    log_file: str = \"./logs/mainrun.log\"\n\ndef configure_logging(log_file: str):\n    Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n\n    file_handler = open(log_file, 'w')\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.PositionalArgumentsFormatter(),\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.UnicodeDecoder(),\n            structlog.processors.JSONRenderer()\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    class DualLogger:\n        def __init__(self, file_handler):\n            self.file_handler = file_handler\n            self.logger = structlog.get_logger()\n\n        def log(self, event, **kwargs):\n            log_entry = json.dumps({\"event\": event, \"timestamp\": time.time(), **kwargs})\n            self.file_handler.write(log_entry + \"\\n\")\n            self.file_handler.flush()\n\n            if kwargs.get(\"prnt\", True):\n                if \"step\" in kwargs and \"max_steps\" in kwargs:\n                    tqdm.write(f\"[{kwargs.get('step'):>5}/{kwargs.get('max_steps')}] {event}: loss={kwargs.get('loss', 'N/A'):.6f} time={kwargs.get('elapsed_time', 0):.2f}s\")\n                else:\n                    parts = [f\"{k}={v}\" for k, v in kwargs.items() if k not in [\"prnt\", \"timestamp\"]]\n                    if parts:\n                        tqdm.write(f\"{event}: {', '.join(parts)}\")\n                    else:\n                        tqdm.write(event)\n\n    return DualLogger(file_handler)\n\nlogger = None\n\ndef get_titles(num_titles: int, seed: int, val_frac: float) -> str:\n    ds = load_dataset(\"julien040/hacker-news-posts\", split=\"train\", cache_dir=\"./data\").shuffle(seed=seed)\n    titles = [row[\"title\"].strip() for row in ds.take(num_titles)]\n    n = int(num_titles * (1 - val_frac))\n    return titles[:n], titles[n:]\n\ndef get_batch(split_ids: torch.Tensor, ptr: int, block_size: int, batch_size: int, device: torch.device):\n    span = block_size * batch_size + 1\n    if ptr + span >= len(split_ids):\n        ptr = 0\n    batch = split_ids[ptr: ptr + span]\n    x = batch[:-1].view(batch_size, block_size).to(device)\n    y = batch[1:].view(batch_size, block_size).to(device)\n    return x, y, ptr + block_size * batch_size\n\ndef iter_full_split(split_ids: torch.Tensor, block_size: int, batch_size: int, device: torch.device):\n    span = block_size * batch_size + 1\n    for ptr in range(0, len(split_ids) - span + 1, span):\n        batch = split_ids[ptr: ptr + span]\n        x = batch[:-1].view(batch_size, block_size).to(device)\n        y = batch[1:].view(batch_size, block_size).to(device)\n        yield x, y\n\ndef train_tokenizer(titles: list[str], vocab_size: int, unk_token: str = \"<unk>\", pad_token: str = \"<pad>\", eos_token: str = \"<eos>\") -> Tokenizer:\n    tokenizer = Tokenizer(models.BPE(unk_token=unk_token))\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n    tokenizer.decoder = decoders.ByteLevel()\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[pad_token, eos_token, unk_token]\n    )\n    tokenizer.train_from_iterator(titles, trainer)\n    return tokenizer\n\nclass BPETokenizer:\n    def __init__(self, tokenizer: Tokenizer):\n        self.tk = tokenizer\n        self.stoi = {tok: i for tok, i in tokenizer.get_vocab().items()}\n        self.itos = {i: tok for tok, i in tokenizer.get_vocab().items()}\n\n    def encode(self, s: str) -> list[int]:\n        return self.tk.encode(s).ids\n\n    def decode(self, ids: list[int]) -> str:\n        return self.tk.decode(ids, skip_special_tokens=True)\n\n    @property\n    def vocab_size(self): return self.tk.get_vocab_size()\n\n@dataclass\nclass GPTConfig:\n    vocab_size: int\n    block_size: int\n    n_layer: int\n    n_head: int\n    d_model: int\n    dropout: float\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        assert cfg.d_model % cfg.n_head == 0\n        self.head_dim = cfg.d_model // cfg.n_head\n        self.n_head   = cfg.n_head\n        self.qkv = nn.Linear(cfg.d_model, 3 * cfg.d_model)\n        self.proj = nn.Linear(cfg.d_model, cfg.d_model)\n        self.attn_drop = nn.Dropout(cfg.dropout)\n        self.resid_drop= nn.Dropout(cfg.dropout)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(cfg.block_size, cfg.block_size)))\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.head_dim).transpose(1, 3)\n        q, k, v = qkv[..., 0, :, :], qkv[..., 1, :, :], qkv[..., 2, :, :]\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        y = att @ v\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return self.resid_drop(self.proj(y))\n\nclass MLP(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(cfg.d_model, 4 * cfg.d_model),\n            nn.GELU(),\n            nn.Linear(4 * cfg.d_model, cfg.d_model),\n            nn.Dropout(cfg.dropout),\n        )\n    def forward(self, x): return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(cfg.d_model)\n        self.ln2 = nn.LayerNorm(cfg.d_model)\n        self.attn = CausalSelfAttention(cfg)\n        self.mlp  = MLP(cfg)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.cfg = cfg\n        self.token_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb   = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.d_model))\n        self.drop      = nn.Dropout(cfg.dropout)\n        self.blocks    = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layer)])\n        self.ln_f      = nn.LayerNorm(cfg.d_model)\n        self.head      = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n        self.apply(self._init_weights)\n        self.head.weight = self.token_emb.weight\n\n    @staticmethod\n    def _init_weights(module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n        B, T = idx.size()\n        tok = self.token_emb(idx)\n        pos = self.pos_emb[:, :T, :]\n        x = self.drop(tok + pos)\n        for block in self.blocks: x = block(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n        if targets is None:\n            loss = None\n        else:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction='mean')\n        return logits, loss\n\n\nargs = Hyperparameters()\ntorch.manual_seed(args.seed)\nrandom.seed(args.seed)\n\nglobal logger\nlogger = configure_logging(args.log_file)\n\nhyperparams_dict = vars(args)\nlogger.log(\"hyperparameters_configured\", **hyperparams_dict)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlogger.log(\"device_info\", device=device)\n\ntrain_titles, val_titles = get_titles(args.num_titles, args.seed, args.val_frac)\n\neos_token = \"<eos>\"\ntok = BPETokenizer(train_tokenizer(train_titles+val_titles, args.vocab_size, eos_token=eos_token))\ntrain_text = eos_token.join(train_titles) + eos_token\nval_text = eos_token.join(val_titles) + eos_token\ntrain_ids = torch.tensor(tok.encode(train_text), dtype=torch.long)\nval_ids = torch.tensor(tok.encode(val_text), dtype=torch.long)\n\nbatches = len(train_ids) // (args.block_size * args.batch_size)\nmax_steps = args.epochs * batches\neval_interval = batches // args.evals_per_epoch\nlogger.log(\"dataset_info\",\n            titles_count=len(train_titles),\n            epochs=args.epochs,\n            batches_per_epoch=batches,\n            tokens_per_epoch=len(train_ids),\n            vocab_size=tok.vocab_size)\n\ncfg = GPTConfig(\n    vocab_size = tok.vocab_size,\n    block_size = args.block_size,\n    n_layer    = args.n_layer,\n    n_head     = args.n_head,\n    d_model    = args.d_model,\n    dropout    = args.dropout,\n)\nmodel = GPT(cfg).to(device)\nmodel_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nlogger.log(\"model_info\", parameters_count=model_params)\n\nopt = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max_steps)\n\ndef evaluate():\n    model.eval()\n    losses = 0.0\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n            logits, _ = model(xb, yb)\n            B, T, V = logits.size()\n            loss = F.cross_entropy(logits.view(-1, V), yb.view(-1), reduction='sum')\n            losses += loss.item()\n    model.train()\n    return losses / len(val_text)\n\nptr = 0\nstep = 0\nt0 = time.time()\nfor epoch in range(1, args.epochs + 1):\n    for _ in tqdm(range(1, batches + 1), desc=f\"Epoch {epoch}/{args.epochs}\"):\n        step += 1\n        xb, yb, ptr = get_batch(train_ids, ptr, args.block_size, args.batch_size, device)\n        _, loss = model(xb, yb)\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        scheduler.step()\n\n        elapsed = time.time() - t0\n        logger.log(\"training_step\",\n                  step=step,\n                  max_steps=max_steps,\n                  loss=loss.item(),\n                  elapsed_time=elapsed,\n                  prnt=False)\n\n        if step == 1 or step % eval_interval == 0 or step == max_steps:\n            val_loss = evaluate()\n            loss_tracker.append(val_loss) # Line added by me\n            logger.log(\"validation_step\",\n                      step=step,\n                      max_steps=max_steps,\n                      loss=val_loss,\n                      elapsed_time=elapsed)","identifier":"1thg0jbjbsc4-code","enumerator":"3","html_id":"id-1thg0jbjbsc4-code","key":"bc2rvnv21c"},{"type":"output","id":"XbarTNGMNnYYTdR999ICg","data":[{"name":"stdout","output_type":"stream","text":"hyperparameters_configured: block_size=128, batch_size=64, vocab_size=16000, n_layer=6, n_head=8, d_model=512, dropout=0.1, lr=0.006, weight_decay=0.0, evals_per_epoch=3, epochs=7, seed=1337, num_titles=100000, val_frac=0.1, log_file=./logs/mainrun.log\ndevice_info: device=cuda\ndataset_info: titles_count=90000, epochs=7, batches_per_epoch=134, tokens_per_epoch=1102455, vocab_size=16000\nmodel_info: parameters_count=27172864\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1/7:   1%|          | 1/134 [00:02<05:40,  2.56s/it]"},{"name":"stdout","output_type":"stream","text":"[    1/938] validation_step: loss=2.097995 time=0.02s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1/7:  33%|███▎      | 44/134 [00:23<01:39,  1.10s/it]"},{"name":"stdout","output_type":"stream","text":"[   44/938] validation_step: loss=1.950199 time=21.22s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1/7:  66%|██████▌   | 88/134 [00:45<00:48,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[   88/938] validation_step: loss=1.887648 time=42.85s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1/7:  99%|█████████▊| 132/134 [01:06<00:02,  1.04s/it]"},{"name":"stdout","output_type":"stream","text":"[  132/938] validation_step: loss=1.842180 time=63.91s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 1/7: 100%|██████████| 134/134 [01:07<00:00,  1.99it/s]\nEpoch 2/7:  31%|███▏      | 42/134 [00:20<01:36,  1.05s/it]"},{"name":"stdout","output_type":"stream","text":"[  176/938] validation_step: loss=1.814669 time=84.91s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 2/7:  64%|██████▍   | 86/134 [00:41<00:50,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  220/938] validation_step: loss=1.798660 time=106.13s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 2/7:  97%|█████████▋| 130/134 [01:02<00:04,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  264/938] validation_step: loss=1.788128 time=127.37s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 2/7: 100%|██████████| 134/134 [01:04<00:00,  2.08it/s]\nEpoch 3/7:  30%|██▉       | 40/134 [00:19<01:38,  1.05s/it]"},{"name":"stdout","output_type":"stream","text":"[  308/938] validation_step: loss=1.780336 time=148.48s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 3/7:  63%|██████▎   | 84/134 [00:40<00:52,  1.05s/it]"},{"name":"stdout","output_type":"stream","text":"[  352/938] validation_step: loss=1.774269 time=169.58s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 3/7:  96%|█████████▌| 128/134 [01:01<00:06,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  396/938] validation_step: loss=1.769562 time=190.71s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 3/7: 100%|██████████| 134/134 [01:04<00:00,  2.09it/s]\nEpoch 4/7:  28%|██▊       | 38/134 [00:18<01:41,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  440/938] validation_step: loss=1.765773 time=211.89s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 4/7:  61%|██████    | 82/134 [00:39<00:54,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  484/938] validation_step: loss=1.762647 time=233.07s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 4/7:  94%|█████████▍| 126/134 [01:00<00:08,  1.05s/it]"},{"name":"stdout","output_type":"stream","text":"[  528/938] validation_step: loss=1.760189 time=254.24s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 4/7: 100%|██████████| 134/134 [01:04<00:00,  2.08it/s]\nEpoch 5/7:  27%|██▋       | 36/134 [00:17<01:43,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  572/938] validation_step: loss=1.758185 time=275.42s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 5/7:  60%|█████▉    | 80/134 [00:38<00:57,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  616/938] validation_step: loss=1.756627 time=296.61s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 5/7:  93%|█████████▎| 124/134 [01:00<00:10,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  660/938] validation_step: loss=1.755449 time=317.77s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 5/7: 100%|██████████| 134/134 [01:04<00:00,  2.08it/s]\nEpoch 6/7:  25%|██▌       | 34/134 [00:16<01:45,  1.05s/it]"},{"name":"stdout","output_type":"stream","text":"[  704/938] validation_step: loss=1.754568 time=338.91s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 6/7:  58%|█████▊    | 78/134 [00:37<00:58,  1.05s/it]"},{"name":"stdout","output_type":"stream","text":"[  748/938] validation_step: loss=1.753978 time=360.05s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 6/7:  91%|█████████ | 122/134 [00:59<00:12,  1.05s/it]"},{"name":"stdout","output_type":"stream","text":"[  792/938] validation_step: loss=1.753598 time=381.22s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 6/7: 100%|██████████| 134/134 [01:04<00:00,  2.08it/s]\nEpoch 7/7:  24%|██▍       | 32/134 [00:15<01:47,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  836/938] validation_step: loss=1.753379 time=402.40s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 7/7:  57%|█████▋    | 76/134 [00:37<01:01,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  880/938] validation_step: loss=1.753293 time=423.58s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 7/7:  90%|████████▉ | 120/134 [00:58<00:14,  1.06s/it]"},{"name":"stdout","output_type":"stream","text":"[  924/938] validation_step: loss=1.753273 time=444.75s\n"},{"name":"stderr","output_type":"stream","text":"Epoch 7/7: 100%|██████████| 134/134 [01:06<00:00,  2.02it/s]"},{"name":"stdout","output_type":"stream","text":"[  938/938] validation_step: loss=1.753273 time=452.91s\n"},{"name":"stderr","output_type":"stream","text":"\n"}],"identifier":"1thg0jbjbsc4-output","html_id":"id-1thg0jbjbsc4-output","key":"YWOSUK4nJU"}],"identifier":"1thg0jbjbsc4","label":"1THg0JBJBsc4","html_id":"id-1thg0jbjbsc4","key":"QzOXmIumfh"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":609},"id":"W8oOHlQ-PzpH","outputId":"987db5ec-5ae9-43d8-9633-d58b0ca4f1ec"},"children":[{"type":"code","lang":"python","executable":true,"value":"# |label: token_evaluate_frequency\n\ndef evaluate_visualise():\n    model.eval()\n    losses = 0.0\n\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n\n            logits, _ = model(xb, yb)\n\n            b = 0 # batch\n            probs = torch.softmax(logits[b], dim=-1)   # (T, V) out\n\n            t = 10  # timestep\n            top_probs, top_ids = probs[t].topk(10)\n\n            # decode token strings\n            decoded_tokens = [tok.tk.decode([tid.item()], skip_special_tokens=False) for tid in top_ids ]\n\n            # Print for debugging\n            print(\"\\nTop-10 predictions at t =\", t)\n            for rank in range(10):\n                print(f\"{rank+1}. p={top_probs[rank].item():.4f} | id={top_ids[rank].item()} | '{decoded_tokens[rank]}'\")\n\n            # plt histogram\n            plt.figure(figsize=(12, 5))\n            plt.bar(decoded_tokens, top_probs.cpu().tolist())\n            plt.title(f\"Top-10 Next-Token Predictions (t={t})\")\n            plt.xlabel(\"Token\")\n            plt.ylabel(\"Probability\")\n            plt.xticks(rotation=45, ha='right')\n            plt.tight_layout()\n            plt.show()\n\n            break  # only first batch\nevaluate_visualise()","identifier":"token_frequency_first_run-code","enumerator":"4","html_id":"token-frequency-first-run-code","key":"epYZwpxAsh"},{"type":"output","id":"sMLlH4ufz9hYkDR5G5CAt","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"2d7caf7c249be1fbc8e7131756ef3219","path":"/2d7caf7c249be1fbc8e7131756ef3219.png"},"text/plain":{"content":"<Figure size 1200x500 with 1 Axes>","content_type":"text/plain"}}}],"identifier":"token_frequency_first_run-output","html_id":"token-frequency-first-run-output","key":"gbhzKYna0M"}],"identifier":"token_frequency_first_run","label":"token_frequency_first_run","html_id":"token-frequency-first-run","key":"cLkSv5MI5I"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":402},"id":"fDpm368rQ54Q","outputId":"732c5cf2-78b8-4ce1-9301-c4c8decc148d"},"children":[{"type":"code","lang":"python","executable":true,"value":"# |label: token_frequency\n\nfrom collections import Counter\n\n# Count frequencies in the entire training corpus\ntoken_freq = Counter(train_ids.tolist())\n\ntop20 = token_freq.most_common(20)\n\n# separate into token IDs and counts\ntoken_ids  = [tid for tid, count in top20]\nfreqs      = [count for tid, count in top20]\ntokens_txt = [tok.tk.decode([tid], skip_special_tokens=False) for tid in token_ids]\n\nplt.figure(figsize=(14,6))\nplt.bar(tokens_txt, freqs)\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Top-20 Most Frequent Tokens in Training Data\")\nplt.xlabel(\"Token\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()","identifier":"token_frequency_input-code","enumerator":"5","html_id":"token-frequency-input-code","key":"MOetwMxkKX"},{"type":"output","id":"JqIlCkfxzVqhEbtxGRrUv","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"95714af3631914dbdd5c918d5cc88961","path":"/95714af3631914dbdd5c918d5cc88961.png"},"text/plain":{"content":"<Figure size 1400x600 with 1 Axes>","content_type":"text/plain"}}}],"identifier":"token_frequency_input-output","html_id":"token-frequency-input-output","key":"G3u4U8XQFC"}],"identifier":"token_frequency_input","label":"token_frequency_input","html_id":"token-frequency-input","key":"FHDDZDd5fR"},{"type":"block","kind":"notebook-code","data":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"Bm8TFtrPCRjX","outputId":"1932593b-f61f-4413-cddc-7567f58de664"},"children":[{"type":"code","lang":"python","executable":true,"value":"# |label: output-first-run\nimport matplotlib.pyplot as plt\n\nplt.plot(loss_tracker)\nplt.show()","identifier":"loss_first_run-code","enumerator":"6","html_id":"loss-first-run-code","key":"I33XoAa4u3"},{"type":"output","id":"cOmpyX8iS7dIibkEYRbq9","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"35bc7d27cd67bfb426903dad2f7d56ba","path":"/35bc7d27cd67bfb426903dad2f7d56ba.png"},"text/plain":{"content":"<Figure size 640x480 with 1 Axes>","content_type":"text/plain"}}}],"identifier":"loss_first_run-output","html_id":"loss-first-run-output","key":"dRWVXPaBI3"}],"identifier":"loss_first_run","label":"loss_first_run","html_id":"loss-first-run","key":"lztjL3vOvY"}],"key":"gWeB2VAEqe"},"references":{"cite":{"order":[],"data":{}}}}