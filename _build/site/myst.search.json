{"version":"1","records":[{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Running train.py using my laptop would (according to my calculations) have taken about 3 days. Instead, I will be using google colab to run the notebook files, which I then download and place in the Notebooks folder of this project. All notebooks can be accessed under ‘supporting documents’.\n\nThis website is build using \n\nMyst, which lets me write about changes I made to train.py in a structured way, combining code-cell outputs with markdown text. For any figure, a source : ... button will appear in the top right corner, clicking it will jump directly to the original notebook file.\n\nThe goal of this project is:\n\nYour task is to analyze and improve the training run of a small GPT-2 style model. The code is designed to train on a Hacker News headline dataset. Your goal is to minimize the validation loss as low as possible within 7 epochs. The baseline to beat is a validation loss of 1.754.\n\nWhere I aim to:\n\nMinimise the validation loss.\n\nUnderstand how well the model performs.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Initial run"},"type":"lvl2","url":"/#initial-run","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Initial run"},"content":"The first step is to run the original train.py file as provided on the GitHub page.\n\n\n\nFigure 1 shows how the loss decreases as training progresses, converging to loss=1.753273 after 7 epochs. We can also have a look at an example of the output provided by the model. For this, I’ll look at batch = 0 and time = 10 within the evaluate_visualise function. The decoded input to the model is:\n\nMotorola Xoom commercial reminiscent of Apple's\n\nThe model’s top-10 predicted next tokens are shown in \n\nFigure 2.\n\n\n\nAs seen in the figure, the model incorrectly predicts <eos>. In fact, the model predicts this token for every input I checked.\n\n\n\nFigure 3 displays the most frequently used words in the input data. This distribution matches the model’s predictions in \n\nFigure 2, which tells us the model is not making the predictions based on context, but rather based on statistics (clever, but now what we are looking for). The aim now is not only to improve the validation loss, but also to adjust the model so it produces more sensible next-token predictions rather than defaulting to <eos>.","type":"content","url":"/#initial-run","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Step one - AdamW Optimiser"},"type":"lvl2","url":"/#step-one-adamw-optimiser","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Step one - AdamW Optimiser"},"content":"The original model uses an SGD optimiser, which is not ideal for training attention-based architectures. I switched to AdamW instead, which is the optimiser typically used for modern transformer and LLM training.# Add different optimiser, which is better suited for transformer models\ndef get_optimizer(model, args):\n    # Add AdamW as an optimiser\n    return torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, betas=args.betas)\n\nI added a separate get_optimizer function so the change is easy to spot at the top of the code cell. I also reduced the learning rate and added explicit beta values for AdamW:n_layer: int = 6\nn_head: int = 8\nd_model: int = 512\ndropout: float = 0.1\nlr = 5e-4                   #lr: float = 6e-3, \nweight_decay: float = 0.0\nevals_per_epoch: int = 3\nbetas = (0.9, 0.95)         # Added betas for AdamW\n\nRunning the model with these changes resulted in a validation loss of 1.32, a noticeable improvement over the original program. More importantly, the model produces far more reasonable next-token suggestions. For example, when given the input:\n\nMotorola Xoom commercial reminiscent of Apple’s\n\nthe predictions look much better, as shown in \n\nFigure 4\n\n\n\nHere, the top prediction is “new”, which fits naturally after the given fragment.","type":"content","url":"/#step-one-adamw-optimiser","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"Step two - Positional context/increased vocab"},"type":"lvl2","url":"/#step-two-positional-context-increased-vocab","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"Step two - Positional context/increased vocab"},"content":"I noticed that each training sample’s context often contains multiple titles in a row, but earlier titles offer no useful information for predicting later ones. For example, the first batch may include an input such as:\n\nMotorola Xoom commercial reminiscent of Apple's infamous jab at IBM<eos> OpenSSH Key Shielding<eos> Contextual Word Representations: A Contextual Introduction (2020)<eos> Windows 8 Should Be Free<eos> GNOME Developers \n\nThe next word after “GNOME Developers” has nothing to do with the earlier titles. Positional information should help the model focus on the current sentence and ignore older ones. I have added \n\nRotary Position Embedding (RoPE) , wwhich I was unfamiliar with before the project but found straightforward to integrate.\n\nIn addition, I increased the tokenizer vocab size and added RSMNorm. Tasks with many unique tokens (technical words, product names, company names, and acronyms) tend to benefit from a larger vocabulary because it reduces over-fragmentation during BPE tokenization.\n\nWith these changes combined, I was able to bring the validation loss down to 1.25. After training, the output distribution for the same input as \n\nFigure 2 is shown below.\n\n\n\nGiven the sentence so far (Motorola Xoom commercial reminiscent of Apple's), the top two predictions (“new” and “first”) are both reasonable continuations.\n\nAnother interesting case is to look at a longer string. By setting b=0 and t=80 in the evaluate_visualise function, we get\n\nMotorola Xoom commercial reminiscent of Apple's infamous jab at IBM<eos> OpenSSH Key Shielding<eos> Contextual Word Representations: A Contextual Introduction (2020)<eos> Windows 8 Should Be Free<eos> GNOME Developers Lay Out Plans for GNOME OS<eos> HTML/CSS/JS Art Hacks<eos> Major Electrical Outtage at The Planet Data Center<eos> South Africa’s Umhlanga Rocks may attract investors<eos>\n\nAt this point, the model must decide how to start a brand-new sentence.\n\n\n\nFigure 6 shows the predicted probability distribution. For the start of a fresh sentence, any of the top-ten tokens would make sense.\n\nThis example also highlights the biggest issue I’ve encountered so far: no amount of tuning can help the model guess the next word (“Snapchat”) without additional context. The model has no information about the underlying article or subject matter, only the previous titles. It simply doesn’t have the necessary cues.","type":"content","url":"/#step-two-positional-context-increased-vocab","position":7},{"hierarchy":{"lvl1":"Introduction","lvl2":"Concluding thoughts"},"type":"lvl2","url":"/#concluding-thoughts","position":8},{"hierarchy":{"lvl1":"Introduction","lvl2":"Concluding thoughts"},"content":"With a final validation loss of 1.25, further improvements are likely to come from providing more context rather than continuing to tune the existing architecture. One promising direction would be to include the article text itself, allowing the model to extract keywords (e.g., company names, locations, products) and general themes before predicting the title.\n\nThis additional information would address the core limitation revealed in the examples: the model often lacks the semantic clues needed to make the “correct” next-token prediction, because that information is nowhere in its current input. Adding this additional context will most likely improve the loss significantly more than small optimisations inside the current pipeline.\n\nUnfortunately, the rules of the challenge don’t allow adding the article text or modifying the dataset, so I couldn’t explore this direction here.","type":"content","url":"/#concluding-thoughts","position":9},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/maincode-step-one","position":0},{"hierarchy":{"lvl1":""},"content":"!pip install torch --index-url https://download.pytorch.org/whl/cu121\n!pip install datasets tokenizers structlog tqdm utils\n\nimport utils\nimport math, random, time\nfrom dataclasses import dataclass\nimport json\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\nfrom tqdm import tqdm\nimport structlog\n\n# CODE ADDED BY ME\n\n#Plotting the loss will allow us to nicely display the loss in a Jupyter Book.\nloss_tracker = []\n\n# Add different optimiser, which is better suited for transformer models\ndef get_optimizer(model, args):\n    # Add AdamW as an optimiser\n    return torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, betas=args.betas)\n\n# CODE FROM GITHUB\n@dataclass\nclass Hyperparameters:\n    block_size: int = 128\n    batch_size: int = 64\n    vocab_size: int = 16_000\n    n_layer: int = 6\n    n_head: int = 8\n    d_model: int = 512\n    dropout: float = 0.1\n    lr = 5e-4                   #lr: float = 6e-3, \n    weight_decay: float = 0.0\n    evals_per_epoch: int = 3\n    betas = (0.9, 0.95)         # Added betas for AdamW\n\n    epochs: int = 7\n    seed: int = 1337\n    num_titles: int = 100_000\n    val_frac: float = 0.10\n    log_file: str = \"./logs/mainrun.log\"\n\ndef configure_logging(log_file: str):\n    Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n\n    file_handler = open(log_file, 'w')\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.PositionalArgumentsFormatter(),\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.UnicodeDecoder(),\n            structlog.processors.JSONRenderer()\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    class DualLogger:\n        def __init__(self, file_handler):\n            self.file_handler = file_handler\n            self.logger = structlog.get_logger()\n\n        def log(self, event, **kwargs):\n            log_entry = json.dumps({\"event\": event, \"timestamp\": time.time(), **kwargs})\n            self.file_handler.write(log_entry + \"\\n\")\n            self.file_handler.flush()\n\n            if kwargs.get(\"prnt\", True):\n                if \"step\" in kwargs and \"max_steps\" in kwargs:\n                    tqdm.write(f\"[{kwargs.get('step'):>5}/{kwargs.get('max_steps')}] {event}: loss={kwargs.get('loss', 'N/A'):.6f} time={kwargs.get('elapsed_time', 0):.2f}s\")\n                else:\n                    parts = [f\"{k}={v}\" for k, v in kwargs.items() if k not in [\"prnt\", \"timestamp\"]]\n                    if parts:\n                        tqdm.write(f\"{event}: {', '.join(parts)}\")\n                    else:\n                        tqdm.write(event)\n\n    return DualLogger(file_handler)\n\nlogger = None\n\ndef get_titles(num_titles: int, seed: int, val_frac: float) -> str:\n    ds = load_dataset(\"julien040/hacker-news-posts\", split=\"train\", cache_dir=\"./data\").shuffle(seed=seed)\n    titles = [row[\"title\"].strip() for row in ds.take(num_titles)]\n    n = int(num_titles * (1 - val_frac))\n    return titles[:n], titles[n:]\n\ndef get_batch(split_ids: torch.Tensor, ptr: int, block_size: int, batch_size: int, device: torch.device):\n    span = block_size * batch_size + 1\n    if ptr + span >= len(split_ids):\n        ptr = 0\n    batch = split_ids[ptr: ptr + span]\n    x = batch[:-1].view(batch_size, block_size).to(device)\n    y = batch[1:].view(batch_size, block_size).to(device)\n    return x, y, ptr + block_size * batch_size\n\ndef iter_full_split(split_ids: torch.Tensor, block_size: int, batch_size: int, device: torch.device):\n    span = block_size * batch_size + 1\n    for ptr in range(0, len(split_ids) - span + 1, span):\n        batch = split_ids[ptr: ptr + span]\n        x = batch[:-1].view(batch_size, block_size).to(device)\n        y = batch[1:].view(batch_size, block_size).to(device)\n        yield x, y\n\ndef train_tokenizer(titles: list[str], vocab_size: int, unk_token: str = \"<unk>\", pad_token: str = \"<pad>\", eos_token: str = \"<eos>\") -> Tokenizer:\n    tokenizer = Tokenizer(models.BPE(unk_token=unk_token))\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n    tokenizer.decoder = decoders.ByteLevel()\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[pad_token, eos_token, unk_token]\n    )\n    tokenizer.train_from_iterator(titles, trainer)\n    return tokenizer\n\nclass BPETokenizer:\n    def __init__(self, tokenizer: Tokenizer):\n        self.tk = tokenizer\n        self.stoi = {tok: i for tok, i in tokenizer.get_vocab().items()}\n        self.itos = {i: tok for tok, i in tokenizer.get_vocab().items()}\n\n    def encode(self, s: str) -> list[int]:\n        return self.tk.encode(s).ids\n\n    def decode(self, ids: list[int]) -> str:\n        return self.tk.decode(ids, skip_special_tokens=True)\n\n    @property\n    def vocab_size(self): return self.tk.get_vocab_size()\n\n@dataclass\nclass GPTConfig:\n    vocab_size: int\n    block_size: int\n    n_layer: int\n    n_head: int\n    d_model: int\n    dropout: float\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        assert cfg.d_model % cfg.n_head == 0\n        self.head_dim = cfg.d_model // cfg.n_head\n        self.n_head   = cfg.n_head\n        self.qkv = nn.Linear(cfg.d_model, 3 * cfg.d_model)\n        self.proj = nn.Linear(cfg.d_model, cfg.d_model)\n        self.attn_drop = nn.Dropout(cfg.dropout)\n        self.resid_drop= nn.Dropout(cfg.dropout)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(cfg.block_size, cfg.block_size)))\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.head_dim).transpose(1, 3)\n        q, k, v = qkv[..., 0, :, :], qkv[..., 1, :, :], qkv[..., 2, :, :]\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        y = att @ v\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return self.resid_drop(self.proj(y))\n\nclass MLP(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(cfg.d_model, 4 * cfg.d_model),\n            nn.GELU(),\n            nn.Linear(4 * cfg.d_model, cfg.d_model),\n            nn.Dropout(cfg.dropout),\n        )\n    def forward(self, x): return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(cfg.d_model)\n        self.ln2 = nn.LayerNorm(cfg.d_model)\n        self.attn = CausalSelfAttention(cfg)\n        self.mlp  = MLP(cfg)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.cfg = cfg\n        self.token_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb   = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.d_model))\n        self.drop      = nn.Dropout(cfg.dropout)\n        self.blocks    = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layer)])\n        self.ln_f      = nn.LayerNorm(cfg.d_model)\n        self.head      = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n        self.apply(self._init_weights)\n        self.head.weight = self.token_emb.weight\n\n    @staticmethod\n    def _init_weights(module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n        B, T = idx.size()\n        tok = self.token_emb(idx)\n        pos = self.pos_emb[:, :T, :]\n        x = self.drop(tok + pos)\n        for block in self.blocks: x = block(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n        if targets is None:\n            loss = None\n        else:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction='mean')\n        return logits, loss\n\nargs = Hyperparameters()\ntorch.manual_seed(args.seed)\nrandom.seed(args.seed)\n\nglobal logger\nlogger = configure_logging(args.log_file)\n\nhyperparams_dict = vars(args)\nlogger.log(\"hyperparameters_configured\", **hyperparams_dict)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlogger.log(\"device_info\", device=device)\n\ntrain_titles, val_titles = get_titles(args.num_titles, args.seed, args.val_frac)\n\neos_token = \"<eos>\"\ntok = BPETokenizer(train_tokenizer(train_titles+val_titles, args.vocab_size, eos_token=eos_token))\ntrain_text = eos_token.join(train_titles) + eos_token\nval_text = eos_token.join(val_titles) + eos_token\ntrain_ids = torch.tensor(tok.encode(train_text), dtype=torch.long)\nval_ids = torch.tensor(tok.encode(val_text), dtype=torch.long)\n\nbatches = len(train_ids) // (args.block_size * args.batch_size)\nmax_steps = args.epochs * batches\neval_interval = batches // args.evals_per_epoch\nlogger.log(\"dataset_info\",\n            titles_count=len(train_titles),\n            epochs=args.epochs,\n            batches_per_epoch=batches,\n            tokens_per_epoch=len(train_ids),\n            vocab_size=tok.vocab_size)\n\ncfg = GPTConfig(\n    vocab_size = tok.vocab_size,\n    block_size = args.block_size,\n    n_layer    = args.n_layer,\n    n_head     = args.n_head,\n    d_model    = args.d_model,\n    dropout    = args.dropout,\n)\nmodel = GPT(cfg).to(device)\nmodel_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nlogger.log(\"model_info\", parameters_count=model_params)\n\nopt = get_optimizer(model, args)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max_steps)\n\ndef evaluate():\n    model.eval()\n    losses = 0.0\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n            logits, _ = model(xb, yb)\n            B, T, V = logits.size()\n            loss = F.cross_entropy(logits.view(-1, V), yb.view(-1), reduction='sum')\n            losses += loss.item()\n    model.train()\n    return losses / len(val_text)\n\nptr = 0\nstep = 0\nt0 = time.time()\nfor epoch in range(1, args.epochs + 1):\n    for _ in tqdm(range(1, batches + 1), desc=f\"Epoch {epoch}/{args.epochs}\"):\n        step += 1\n        xb, yb, ptr = get_batch(train_ids, ptr, args.block_size, args.batch_size, device)\n        _, loss = model(xb, yb)\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        scheduler.step()\n\n        elapsed = time.time() - t0\n        logger.log(\"training_step\",\n                  step=step,\n                  max_steps=max_steps,\n                  loss=loss.item(),\n                  elapsed_time=elapsed,\n                  prnt=False)\n\n        if step == 1 or step % eval_interval == 0 or step == max_steps:\n            val_loss = evaluate()\n            loss_tracker.append(val_loss) # Line added by me\n            logger.log(\"validation_step\",\n                      step=step,\n                      max_steps=max_steps,\n                      loss=val_loss,\n                      elapsed_time=elapsed)\n\n# |label: token_evaluate_frequency_first_step\nimport matplotlib.pyplot as plt\n\n# Function to evaluate and visualise top-10 token predictions for a given input from the validation set\ndef evaluate_visualise():\n    \n    model.eval()\n\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n\n            logits, _ = model(xb, yb)\n\n            b = 0 # batch\n            t = 10  # timestep\n\n            probs = torch.softmax(logits[b], dim=-1)   # (T, V) out\n\n            # top 10 tokens\n            top_probs, top_ids = probs[t].topk(10)\n\n            # decode token strings\n            decoded_tokens = [tok.tk.decode([tid.item()], skip_special_tokens=False) for tid in top_ids ]\n\n            # Print for debugging\n            print(\"\\nTop-10 predictions at t =\", t)\n            for rank in range(10):\n                print(f\"{rank+1}. p={top_probs[rank].item():.4f} | id={top_ids[rank].item()} | '{decoded_tokens[rank]}'\")\n\n            # plt histogram\n            plt.figure(figsize=(12, 5))\n            plt.bar(decoded_tokens, top_probs.cpu().tolist())\n            plt.title(f\"Top-10 Next-Token Predictions (t={t})\")\n            plt.xlabel(\"Token\")\n            plt.ylabel(\"Probability\")\n            plt.xticks(rotation=45, ha='right')\n            plt.tight_layout()\n            plt.show()\n\n            break  # only once\n\n\nevaluate_visualise()\n\n# |label: output-first-step\nimport matplotlib.pyplot as plt\n\nplt.plot(loss_tracker)\nplt.show()","type":"content","url":"/maincode-step-one","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/maincode-step-two","position":0},{"hierarchy":{"lvl1":""},"content":"!pip install torch --index-url https://download.pytorch.org/whl/cu121\n!pip install datasets tokenizers structlog tqdm utils\n\nimport utils\nimport math, random, time\nfrom dataclasses import dataclass\nimport json\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\nfrom tqdm import tqdm\nimport structlog\n\n# CODE ADDED BY ME\nfrom torch.nn import RMSNorm\n\n#Plotting the loss will allow us to nicely display the loss in a Jupyter Book.\nloss_tracker = []\n\n# Add different optimiser, which is better suited for transformer models\ndef get_optimizer(model, args):\n    return torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, betas=args.betas)\n\n# Add learning rate scheduler (currently still the same as before)\ndef get_scheduler(opt):\n  return torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max_steps)\n\n# Use RMSNorm instead of LayerNorm - more efficient for transformer models\ndef get_norm(d_model):\n  return RMSNorm(d_model)\n\n# RoPE implementation, allows the model to better capture positional information\ndef apply_rope(x):\n    # x shape: (B, n_head, T, head_dim)\n    B, H, T, D = x.shape\n    half = D // 2\n\n    # Split into two halves\n    x1 = x[..., :half]\n    x2 = x[..., half:]\n\n    # Create rotation frequencies\n    theta = 1.0 / (10000 ** (torch.arange(0, half, device=x.device) / half)) # 10000 given online\n    pos = torch.arange(T, device=x.device).unsqueeze(1)\n\n    # Compute angles\n    angle = pos * theta  # (T, half)\n\n    cos = angle.cos()[None, None, :, :]   # (1,1,T,half)\n    sin = angle.sin()[None, None, :, :]   # (1,1,T,half)\n\n    # Apply rotation\n    x_rotated = torch.cat([x1 * cos - x2 * sin,\n                           x1 * sin + x2 * cos], dim=-1)\n    return x_rotated\n\n# CODE FROM GITHUB\n@dataclass\nclass Hyperparameters:\n    block_size: int = 128\n    batch_size: int = 64\n    vocab_size: int = 32_000\n    n_layer = 4\n    n_head  = 4\n    d_model = 512\n    dropout: float = 0.1\n    lr = 5e-4                   #lr: float = 6e-3, decreased\n    weight_decay: float = 0.1\n    evals_per_epoch: int = 3\n    betas = (0.9, 0.99)         # Added betas (increased)\n\n    epochs: int = 7\n    seed: int = 1337\n    num_titles: int = 100_000\n    val_frac: float = 0.10\n    log_file: str = \"./logs/mainrun.log\"\n\ndef configure_logging(log_file: str):\n    Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n\n    file_handler = open(log_file, 'w')\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.PositionalArgumentsFormatter(),\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.UnicodeDecoder(),\n            structlog.processors.JSONRenderer()\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    class DualLogger:\n        def __init__(self, file_handler):\n            self.file_handler = file_handler\n            self.logger = structlog.get_logger()\n\n        def log(self, event, **kwargs):\n            log_entry = json.dumps({\"event\": event, \"timestamp\": time.time(), **kwargs})\n            self.file_handler.write(log_entry + \"\\n\")\n            self.file_handler.flush()\n\n            if kwargs.get(\"prnt\", True):\n                if \"step\" in kwargs and \"max_steps\" in kwargs:\n                    tqdm.write(f\"[{kwargs.get('step'):>5}/{kwargs.get('max_steps')}] {event}: loss={kwargs.get('loss', 'N/A'):.6f} time={kwargs.get('elapsed_time', 0):.2f}s\")\n                else:\n                    parts = [f\"{k}={v}\" for k, v in kwargs.items() if k not in [\"prnt\", \"timestamp\"]]\n                    if parts:\n                        tqdm.write(f\"{event}: {', '.join(parts)}\")\n                    else:\n                        tqdm.write(event)\n\n    return DualLogger(file_handler)\n\nlogger = None\n\ndef get_titles(num_titles: int, seed: int, val_frac: float) -> str:\n    ds = load_dataset(\"julien040/hacker-news-posts\", split=\"train\", cache_dir=\"./data\").shuffle(seed=seed)\n    titles = [row[\"title\"].strip() for row in ds.take(num_titles)]\n    n = int(num_titles * (1 - val_frac))\n    return titles[:n], titles[n:]\n\ndef get_batch(split_ids: torch.Tensor, ptr: int, block_size: int, batch_size: int, device: torch.device):\n    span = block_size * batch_size + 1\n    if ptr + span >= len(split_ids):\n        ptr = 0\n    batch = split_ids[ptr: ptr + span]\n    x = batch[:-1].view(batch_size, block_size).to(device)\n    y = batch[1:].view(batch_size, block_size).to(device)\n    return x, y, ptr + block_size * batch_size\n\ndef iter_full_split(split_ids: torch.Tensor, block_size: int, batch_size: int, device: torch.device):\n    span = block_size * batch_size + 1\n    for ptr in range(0, len(split_ids) - span + 1, span):\n        batch = split_ids[ptr: ptr + span]\n        x = batch[:-1].view(batch_size, block_size).to(device)\n        y = batch[1:].view(batch_size, block_size).to(device)\n        yield x, y\n\ndef train_tokenizer(titles: list[str], vocab_size: int, unk_token: str = \"<unk>\", pad_token: str = \"<pad>\", eos_token: str = \"<eos>\") -> Tokenizer:\n    tokenizer = Tokenizer(models.BPE(unk_token=unk_token))\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n    tokenizer.decoder = decoders.ByteLevel()\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[pad_token, eos_token, unk_token]\n    )\n    tokenizer.train_from_iterator(titles, trainer)\n    return tokenizer\n\nclass BPETokenizer:\n    def __init__(self, tokenizer: Tokenizer):\n        self.tk = tokenizer\n        self.stoi = {tok: i for tok, i in tokenizer.get_vocab().items()}\n        self.itos = {i: tok for tok, i in tokenizer.get_vocab().items()}\n\n    def encode(self, s: str) -> list[int]:\n        return self.tk.encode(s).ids\n\n    def decode(self, ids: list[int]) -> str:\n        return self.tk.decode(ids, skip_special_tokens=True)\n\n    @property\n    def vocab_size(self): return self.tk.get_vocab_size()\n\n@dataclass\nclass GPTConfig:\n    vocab_size: int\n    block_size: int\n    n_layer: int\n    n_head: int\n    d_model: int\n    dropout: float\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        assert cfg.d_model % cfg.n_head == 0\n        self.head_dim = cfg.d_model // cfg.n_head\n        self.n_head   = cfg.n_head\n        self.qkv = nn.Linear(cfg.d_model, 3 * cfg.d_model)\n        self.proj = nn.Linear(cfg.d_model, cfg.d_model)\n        self.attn_drop = nn.Dropout(cfg.dropout)\n        self.resid_drop= nn.Dropout(cfg.dropout)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(cfg.block_size, cfg.block_size)))\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.head_dim).transpose(1, 3)\n        q, k, v = qkv[..., 0, :, :], qkv[..., 1, :, :], qkv[..., 2, :, :]\n        q = apply_rope(q)\n        k = apply_rope(k)\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        y = att @ v\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return self.resid_drop(self.proj(y))\n\nclass MLP(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(cfg.d_model, 8 * cfg.d_model),\n            nn.GELU(),\n            nn.Linear(8 * cfg.d_model, cfg.d_model),\n            nn.Dropout(cfg.dropout),\n        )\n    def forward(self, x): return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.ln1 = get_norm(cfg.d_model)\n        self.ln2 = get_norm(cfg.d_model)\n        self.attn = CausalSelfAttention(cfg)\n        self.mlp  = MLP(cfg)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.cfg = cfg\n        self.token_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb   = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.d_model))\n        self.drop      = nn.Dropout(cfg.dropout)\n        self.blocks    = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layer)])\n        self.ln_f      = get_norm(cfg.d_model)\n        self.head      = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n        self.apply(self._init_weights)\n        self.head.weight = self.token_emb.weight\n\n    @staticmethod\n    def _init_weights(module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n        B, T = idx.size()\n        tok = self.token_emb(idx)\n        pos = self.pos_emb[:, :T, :]\n        x = self.drop(tok + pos)\n        for block in self.blocks: x = block(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n        if targets is None:\n            loss = None\n        else:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction='mean')\n        return logits, loss\n\nargs = Hyperparameters()\ntorch.manual_seed(args.seed)\nrandom.seed(args.seed)\n\nglobal logger\nlogger = configure_logging(args.log_file)\n\nhyperparams_dict = vars(args)\nlogger.log(\"hyperparameters_configured\", **hyperparams_dict)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlogger.log(\"device_info\", device=device)\n\ntrain_titles, val_titles = get_titles(args.num_titles, args.seed, args.val_frac)\n\neos_token = \"<eos>\"\ntok = BPETokenizer(train_tokenizer(train_titles+val_titles, args.vocab_size, eos_token=eos_token))\ntrain_text = eos_token.join(train_titles) + eos_token\nval_text = eos_token.join(val_titles) + eos_token\ntrain_ids = torch.tensor(tok.encode(train_text), dtype=torch.long)\nval_ids = torch.tensor(tok.encode(val_text), dtype=torch.long)\n\nbatches = len(train_ids) // (args.block_size * args.batch_size)\nmax_steps = args.epochs * batches\neval_interval = batches // args.evals_per_epoch\nlogger.log(\"dataset_info\",\n            titles_count=len(train_titles),\n            epochs=args.epochs,\n            batches_per_epoch=batches,\n            tokens_per_epoch=len(train_ids),\n            vocab_size=tok.vocab_size)\n\ncfg = GPTConfig(\n    vocab_size = tok.vocab_size,\n    block_size = args.block_size,\n    n_layer    = args.n_layer,\n    n_head     = args.n_head,\n    d_model    = args.d_model,\n    dropout    = args.dropout,\n)\nmodel = GPT(cfg).to(device)\nmodel_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nlogger.log(\"model_info\", parameters_count=model_params)\n\nopt = get_optimizer(model, args)\nscheduler = get_scheduler(opt)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max_steps)\n\ndef evaluate():\n    model.eval()\n    losses = 0.0\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n            logits, _ = model(xb, yb)\n            B, T, V = logits.size()\n            loss = F.cross_entropy(logits.view(-1, V), yb.view(-1), reduction='sum')\n            losses += loss.item()\n    model.train()\n    return losses / len(val_text)\n\nptr = 0\nstep = 0\nt0 = time.time()\nfor epoch in range(1, args.epochs + 1):\n    for _ in tqdm(range(1, batches + 1), desc=f\"Epoch {epoch}/{args.epochs}\"):\n        step += 1\n        xb, yb, ptr = get_batch(train_ids, ptr, args.block_size, args.batch_size, device)\n        _, loss = model(xb, yb)\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        scheduler.step()\n\n        elapsed = time.time() - t0\n        logger.log(\"training_step\",\n                  step=step,\n                  max_steps=max_steps,\n                  loss=loss.item(),\n                  elapsed_time=elapsed,\n                  prnt=False)\n\n        if step == 1 or step % eval_interval == 0 or step == max_steps:\n            val_loss = evaluate()\n            loss_tracker.append(val_loss) # Line added by me\n            logger.log(\"validation_step\",\n                      step=step,\n                      max_steps=max_steps,\n                      loss=val_loss,\n                      elapsed_time=elapsed)\n\n# |label: token_evaluate_frequency_step_two\nimport matplotlib.pyplot as plt\n\ndef evaluate_visualise():\n    model.eval()\n    losses = 0.0\n\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n\n            logits, _ = model(xb, yb)\n\n            b = 0 # batch\n            t = 80  # timestep\n\n            probs = torch.softmax(logits[b], dim=-1)   # (T, V) out\n\n\n            # displat context\n            prefix_ids = xb[b, :t+1].tolist()\n            prefix_str = tok.tk.decode(prefix_ids, skip_special_tokens=False)\n            print(f\"\\n[CONTEXT up to t={t}]\")\n            print(prefix_str)\n\n            top_probs, top_ids = probs[t].topk(10)\n\n            # decode token strings\n            decoded_tokens = [tok.tk.decode([tid.item()], skip_special_tokens=False) for tid in top_ids ]\n\n            # Print for debugging\n            print(\"\\nTop-10 predictions at t =\", t)\n            for rank in range(10):\n                print(f\"{rank+1}. p={top_probs[rank].item():.4f} | id={top_ids[rank].item()} | '{decoded_tokens[rank]}'\")\n\n            # plt histogram\n            plt.figure(figsize=(12, 5))\n            plt.bar(decoded_tokens, top_probs.cpu().tolist())\n            plt.title(f\"Top-10 Next-Token Predictions (t={t})\")\n            plt.xlabel(\"Token\")\n            plt.ylabel(\"Probability\")\n            plt.xticks(rotation=45, ha='right')\n            plt.tight_layout()\n            plt.show()\n\n            break  # only first batch\nevaluate_visualise()\n\n# |label: token_evaluate_frequency_step_two\nimport matplotlib.pyplot as plt\n\ndef evaluate_visualise():\n    model.eval()\n    losses = 0.0\n\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n\n            logits, _ = model(xb, yb)\n\n            b = 0 # batch\n            t = 10  # timestep\n\n            probs = torch.softmax(logits[b], dim=-1)   # (T, V) out\n\n\n            # displat context\n            prefix_ids = xb[b, :t+1].tolist()\n            prefix_str = tok.tk.decode(prefix_ids, skip_special_tokens=False)\n            print(f\"\\n[CONTEXT up to t={t}]\")\n            print(prefix_str)\n\n            top_probs, top_ids = probs[t].topk(10)\n\n            # decode token strings\n            decoded_tokens = [tok.tk.decode([tid.item()], skip_special_tokens=False) for tid in top_ids ]\n\n            # Print for debugging\n            print(\"\\nTop-10 predictions at t =\", t)\n            for rank in range(10):\n                print(f\"{rank+1}. p={top_probs[rank].item():.4f} | id={top_ids[rank].item()} | '{decoded_tokens[rank]}'\")\n\n            # plt histogram\n            plt.figure(figsize=(12, 5))\n            plt.bar(decoded_tokens, top_probs.cpu().tolist())\n            plt.title(f\"Top-10 Next-Token Predictions (t={t})\")\n            plt.xlabel(\"Token\")\n            plt.ylabel(\"Probability\")\n            plt.xticks(rotation=45, ha='right')\n            plt.tight_layout()\n            plt.show()\n\n            break  # only first batch\nevaluate_visualise()\n\n# |label: output-second-step\nimport matplotlib.pyplot as plt\n\nplt.plot(loss_tracker)\nplt.show()","type":"content","url":"/maincode-step-two","position":1},{"hierarchy":{"lvl1":"Mainrun notebook - First run"},"type":"lvl1","url":"/maincode-first-run","position":0},{"hierarchy":{"lvl1":"Mainrun notebook - First run"},"content":"This is the first run of the code, without any changes. This is what I will be improving upon in the following notebooks.\n\n!pip install torch --index-url https://download.pytorch.org/whl/cu121\n!pip install datasets tokenizers structlog tqdm utils\n\nimport utils\nimport math, random, time\nfrom dataclasses import dataclass\nimport json\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\nfrom tqdm import tqdm\nimport structlog\n\n# CODE ADDED BY ME FOR LOGGING\n\nloss_tracker = [] #Plotting the loss will allow us to nicely display the loss in a Jupyter Book.\n\n# CODE FROM GITHUB\n@dataclass\nclass Hyperparameters:\n    block_size: int = 128\n    batch_size: int = 64\n    vocab_size: int = 16_000\n    n_layer: int = 6\n    n_head: int = 8\n    d_model: int = 512\n    dropout: float = 0.1\n    lr: float = 6e-3\n    weight_decay: float = 0.0\n    evals_per_epoch: int = 3\n\n    epochs: int = 7\n    seed: int = 1337\n    num_titles: int = 100_000\n    val_frac: float = 0.10\n    log_file: str = \"./logs/mainrun.log\"\n\ndef configure_logging(log_file: str):\n    Path(log_file).parent.mkdir(parents=True, exist_ok=True)\n\n    file_handler = open(log_file, 'w')\n\n    structlog.configure(\n        processors=[\n            structlog.stdlib.filter_by_level,\n            structlog.stdlib.add_logger_name,\n            structlog.stdlib.add_log_level,\n            structlog.stdlib.PositionalArgumentsFormatter(),\n            structlog.processors.TimeStamper(fmt=\"iso\"),\n            structlog.processors.StackInfoRenderer(),\n            structlog.processors.format_exc_info,\n            structlog.processors.UnicodeDecoder(),\n            structlog.processors.JSONRenderer()\n        ],\n        context_class=dict,\n        logger_factory=structlog.stdlib.LoggerFactory(),\n        cache_logger_on_first_use=True,\n    )\n\n    class DualLogger:\n        def __init__(self, file_handler):\n            self.file_handler = file_handler\n            self.logger = structlog.get_logger()\n\n        def log(self, event, **kwargs):\n            log_entry = json.dumps({\"event\": event, \"timestamp\": time.time(), **kwargs})\n            self.file_handler.write(log_entry + \"\\n\")\n            self.file_handler.flush()\n\n            if kwargs.get(\"prnt\", True):\n                if \"step\" in kwargs and \"max_steps\" in kwargs:\n                    tqdm.write(f\"[{kwargs.get('step'):>5}/{kwargs.get('max_steps')}] {event}: loss={kwargs.get('loss', 'N/A'):.6f} time={kwargs.get('elapsed_time', 0):.2f}s\")\n                else:\n                    parts = [f\"{k}={v}\" for k, v in kwargs.items() if k not in [\"prnt\", \"timestamp\"]]\n                    if parts:\n                        tqdm.write(f\"{event}: {', '.join(parts)}\")\n                    else:\n                        tqdm.write(event)\n\n    return DualLogger(file_handler)\n\nlogger = None\n\ndef get_titles(num_titles: int, seed: int, val_frac: float) -> str:\n    ds = load_dataset(\"julien040/hacker-news-posts\", split=\"train\", cache_dir=\"./data\").shuffle(seed=seed)\n    titles = [row[\"title\"].strip() for row in ds.take(num_titles)]\n    n = int(num_titles * (1 - val_frac))\n    return titles[:n], titles[n:]\n\ndef get_batch(split_ids: torch.Tensor, ptr: int, block_size: int, batch_size: int, device: torch.device):\n    span = block_size * batch_size + 1\n    if ptr + span >= len(split_ids):\n        ptr = 0\n    batch = split_ids[ptr: ptr + span]\n    x = batch[:-1].view(batch_size, block_size).to(device)\n    y = batch[1:].view(batch_size, block_size).to(device)\n    return x, y, ptr + block_size * batch_size\n\ndef iter_full_split(split_ids: torch.Tensor, block_size: int, batch_size: int, device: torch.device):\n    span = block_size * batch_size + 1\n    for ptr in range(0, len(split_ids) - span + 1, span):\n        batch = split_ids[ptr: ptr + span]\n        x = batch[:-1].view(batch_size, block_size).to(device)\n        y = batch[1:].view(batch_size, block_size).to(device)\n        yield x, y\n\ndef train_tokenizer(titles: list[str], vocab_size: int, unk_token: str = \"<unk>\", pad_token: str = \"<pad>\", eos_token: str = \"<eos>\") -> Tokenizer:\n    tokenizer = Tokenizer(models.BPE(unk_token=unk_token))\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n    tokenizer.decoder = decoders.ByteLevel()\n    trainer = trainers.BpeTrainer(\n        vocab_size=vocab_size,\n        special_tokens=[pad_token, eos_token, unk_token]\n    )\n    tokenizer.train_from_iterator(titles, trainer)\n    return tokenizer\n\nclass BPETokenizer:\n    def __init__(self, tokenizer: Tokenizer):\n        self.tk = tokenizer\n        self.stoi = {tok: i for tok, i in tokenizer.get_vocab().items()}\n        self.itos = {i: tok for tok, i in tokenizer.get_vocab().items()}\n\n    def encode(self, s: str) -> list[int]:\n        return self.tk.encode(s).ids\n\n    def decode(self, ids: list[int]) -> str:\n        return self.tk.decode(ids, skip_special_tokens=True)\n\n    @property\n    def vocab_size(self): return self.tk.get_vocab_size()\n\n@dataclass\nclass GPTConfig:\n    vocab_size: int\n    block_size: int\n    n_layer: int\n    n_head: int\n    d_model: int\n    dropout: float\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        assert cfg.d_model % cfg.n_head == 0\n        self.head_dim = cfg.d_model // cfg.n_head\n        self.n_head   = cfg.n_head\n        self.qkv = nn.Linear(cfg.d_model, 3 * cfg.d_model)\n        self.proj = nn.Linear(cfg.d_model, cfg.d_model)\n        self.attn_drop = nn.Dropout(cfg.dropout)\n        self.resid_drop= nn.Dropout(cfg.dropout)\n        self.register_buffer(\"tril\", torch.tril(torch.ones(cfg.block_size, cfg.block_size)))\n\n    def forward(self, x: torch.Tensor):\n        B, T, C = x.size()\n        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.head_dim).transpose(1, 3)\n        q, k, v = qkv[..., 0, :, :], qkv[..., 1, :, :], qkv[..., 2, :, :]\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n        y = att @ v\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        return self.resid_drop(self.proj(y))\n\nclass MLP(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(cfg.d_model, 4 * cfg.d_model),\n            nn.GELU(),\n            nn.Linear(4 * cfg.d_model, cfg.d_model),\n            nn.Dropout(cfg.dropout),\n        )\n    def forward(self, x): return self.net(x)\n\nclass Block(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(cfg.d_model)\n        self.ln2 = nn.LayerNorm(cfg.d_model)\n        self.attn = CausalSelfAttention(cfg)\n        self.mlp  = MLP(cfg)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, cfg: GPTConfig):\n        super().__init__()\n        self.cfg = cfg\n        self.token_emb = nn.Embedding(cfg.vocab_size, cfg.d_model)\n        self.pos_emb   = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.d_model))\n        self.drop      = nn.Dropout(cfg.dropout)\n        self.blocks    = nn.ModuleList([Block(cfg) for _ in range(cfg.n_layer)])\n        self.ln_f      = nn.LayerNorm(cfg.d_model)\n        self.head      = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n\n        self.apply(self._init_weights)\n        self.head.weight = self.token_emb.weight\n\n    @staticmethod\n    def _init_weights(module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n        B, T = idx.size()\n        tok = self.token_emb(idx)\n        pos = self.pos_emb[:, :T, :]\n        x = self.drop(tok + pos)\n        for block in self.blocks: x = block(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n        if targets is None:\n            loss = None\n        else:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction='mean')\n        return logits, loss\n\n\nargs = Hyperparameters()\ntorch.manual_seed(args.seed)\nrandom.seed(args.seed)\n\nglobal logger\nlogger = configure_logging(args.log_file)\n\nhyperparams_dict = vars(args)\nlogger.log(\"hyperparameters_configured\", **hyperparams_dict)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlogger.log(\"device_info\", device=device)\n\ntrain_titles, val_titles = get_titles(args.num_titles, args.seed, args.val_frac)\n\neos_token = \"<eos>\"\ntok = BPETokenizer(train_tokenizer(train_titles+val_titles, args.vocab_size, eos_token=eos_token))\ntrain_text = eos_token.join(train_titles) + eos_token\nval_text = eos_token.join(val_titles) + eos_token\ntrain_ids = torch.tensor(tok.encode(train_text), dtype=torch.long)\nval_ids = torch.tensor(tok.encode(val_text), dtype=torch.long)\n\nbatches = len(train_ids) // (args.block_size * args.batch_size)\nmax_steps = args.epochs * batches\neval_interval = batches // args.evals_per_epoch\nlogger.log(\"dataset_info\",\n            titles_count=len(train_titles),\n            epochs=args.epochs,\n            batches_per_epoch=batches,\n            tokens_per_epoch=len(train_ids),\n            vocab_size=tok.vocab_size)\n\ncfg = GPTConfig(\n    vocab_size = tok.vocab_size,\n    block_size = args.block_size,\n    n_layer    = args.n_layer,\n    n_head     = args.n_head,\n    d_model    = args.d_model,\n    dropout    = args.dropout,\n)\nmodel = GPT(cfg).to(device)\nmodel_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nlogger.log(\"model_info\", parameters_count=model_params)\n\nopt = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max_steps)\n\ndef evaluate():\n    model.eval()\n    losses = 0.0\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n            logits, _ = model(xb, yb)\n            B, T, V = logits.size()\n            loss = F.cross_entropy(logits.view(-1, V), yb.view(-1), reduction='sum')\n            losses += loss.item()\n    model.train()\n    return losses / len(val_text)\n\nptr = 0\nstep = 0\nt0 = time.time()\nfor epoch in range(1, args.epochs + 1):\n    for _ in tqdm(range(1, batches + 1), desc=f\"Epoch {epoch}/{args.epochs}\"):\n        step += 1\n        xb, yb, ptr = get_batch(train_ids, ptr, args.block_size, args.batch_size, device)\n        _, loss = model(xb, yb)\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        scheduler.step()\n\n        elapsed = time.time() - t0\n        logger.log(\"training_step\",\n                  step=step,\n                  max_steps=max_steps,\n                  loss=loss.item(),\n                  elapsed_time=elapsed,\n                  prnt=False)\n\n        if step == 1 or step % eval_interval == 0 or step == max_steps:\n            val_loss = evaluate()\n            loss_tracker.append(val_loss) # Line added by me\n            logger.log(\"validation_step\",\n                      step=step,\n                      max_steps=max_steps,\n                      loss=val_loss,\n                      elapsed_time=elapsed)\n\n# |label: token_evaluate_frequency\n\ndef evaluate_visualise():\n    model.eval()\n    losses = 0.0\n\n    with torch.no_grad():\n        for xb, yb in iter_full_split(val_ids, args.block_size, args.batch_size, device):\n\n            logits, _ = model(xb, yb)\n\n            b = 0 # batch\n            probs = torch.softmax(logits[b], dim=-1)   # (T, V) out\n\n            t = 10  # timestep\n            top_probs, top_ids = probs[t].topk(10)\n\n            # decode token strings\n            decoded_tokens = [tok.tk.decode([tid.item()], skip_special_tokens=False) for tid in top_ids ]\n\n            # Print for debugging\n            print(\"\\nTop-10 predictions at t =\", t)\n            for rank in range(10):\n                print(f\"{rank+1}. p={top_probs[rank].item():.4f} | id={top_ids[rank].item()} | '{decoded_tokens[rank]}'\")\n\n            # plt histogram\n            plt.figure(figsize=(12, 5))\n            plt.bar(decoded_tokens, top_probs.cpu().tolist())\n            plt.title(f\"Top-10 Next-Token Predictions (t={t})\")\n            plt.xlabel(\"Token\")\n            plt.ylabel(\"Probability\")\n            plt.xticks(rotation=45, ha='right')\n            plt.tight_layout()\n            plt.show()\n\n            break  # only first batch\nevaluate_visualise()\n\n# |label: token_frequency\n\nfrom collections import Counter\n\n# Count frequencies in the entire training corpus\ntoken_freq = Counter(train_ids.tolist())\n\ntop20 = token_freq.most_common(20)\n\n# separate into token IDs and counts\ntoken_ids  = [tid for tid, count in top20]\nfreqs      = [count for tid, count in top20]\ntokens_txt = [tok.tk.decode([tid], skip_special_tokens=False) for tid in token_ids]\n\nplt.figure(figsize=(14,6))\nplt.bar(tokens_txt, freqs)\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Top-20 Most Frequent Tokens in Training Data\")\nplt.xlabel(\"Token\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n# |label: output-first-run\nimport matplotlib.pyplot as plt\n\nplt.plot(loss_tracker)\nplt.show()","type":"content","url":"/maincode-first-run","position":1}]}